{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "\n",
    "inpath = \"../../\"\n",
    "\n",
    "currentfile = \"Imitator_2_2400.csv\"\n",
    "\n",
    "# Read from file\n",
    "strdatatype = np.dtype([('N', np.int_, (2,)), ('Time_Count', np.int_ ), ('Mode', np.int_ ),\n",
    "                            ('T', np.float_, (10,)), ('S', np.bool_, (10,)), ('System_State', np.bool_ )])\n",
    "N, Time_Count, Mode, T, S, System_State = np.loadtxt(path.join(inpath, currentfile),\n",
    "        unpack=True, delimiter=';', skiprows=1, dtype=strdatatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_enc(_data_full, _now, _depth):\n",
    "    '''\n",
    "    returns slice of inintial array to pass it to LSTM input on one given timestamp\n",
    "    '''\n",
    "    data_slice = _data_full[_now - _depth: _now,:]\n",
    "    return data_slice\n",
    "\n",
    "def slice_pred(_data_full, _now, _depth):\n",
    "    '''\n",
    "    returns slice of inintial array to pass it to LSTM input on one given timestamp\n",
    "    '''\n",
    "    data_slice = _data_full[_now: _now + _depth]\n",
    "    return data_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_steps = 20\n",
    "\n",
    "first_n = N[0,0] \n",
    "last_n = N[-1,-1]\n",
    "\n",
    "train_enc_start\n",
    "train_enc_end\n",
    "train_pred_start\n",
    "train_pred_end\n",
    "\n",
    "val_enc_start\n",
    "val_enc_end\n",
    "val_pred_start\n",
    "val_pred_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50 # LSTM hidden units\n",
    "dropout = .20 \n",
    "\n",
    "# Define an input series and encode it with an LSTM. \n",
    "encoder_inputs = Input(shape=(None, 1)) \n",
    "encoder = LSTM(latent_dim, dropout=dropout, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the final states. These represent the \"context\"\n",
    "# vector that we use as the basis for decoding.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "# This is where teacher forcing inputs are fed in.\n",
    "decoder_inputs = Input(shape=(None, 1)) \n",
    "\n",
    "# We set up our decoder using `encoder_states` as initial state.  \n",
    "# We return full output sequences and return internal states as well. \n",
    "# We don't use the return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, dropout=dropout, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(1) # 1 continuous output at each timestep\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
