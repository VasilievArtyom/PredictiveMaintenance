{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import signal\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, GRU, Embedding, Dense, Activation, Dropout, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "\n",
    "outpath = \"../../../plots/temperatures\"\n",
    "inpath = \"../../../\"\n",
    "\n",
    "currentfile = \"Imitator_2_2400.csv\"\n",
    "\n",
    "# Read from file\n",
    "strdatatype = np.dtype([('N', np.int_, (2,)), ('Time_Count', np.int_ ), ('Mode', np.int_ ),\n",
    "                        ('T', np.float_, (10,)), ('S', np.bool_, (10,)), ('System_State', np.bool_ )])\n",
    "N, Time_Count, Mode, T, S, System_State = np.loadtxt(path.join(inpath, currentfile), \n",
    "                                                     unpack=True, delimiter=';', skiprows=1, dtype=strdatatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_possible_model_temperature = 15.0\n",
    "min_possible_model_temperature = 70.0\n",
    "def scale_T(_T):\n",
    "    return (_T - min_possible_model_temperature) / (max_possible_model_temperature - min_possible_model_temperature)\n",
    "\n",
    "def unscale_T(_T):\n",
    "    return (_T) * (max_possible_model_temperature - min_possible_model_temperature) + min_possible_model_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sT = scale_T(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0883266748985012\n",
      "Max: 0.9380161784282788\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", np.min(sT))\n",
    "print(\"Max:\", np.max(sT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1317509536489254e-06"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = np.amin(np.abs(sT[:-2, :] - sT[1:-1, :])) \n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmintation -- 50000 slightly tuned copies of T\n",
    "agmntCount=5\n",
    "agmntdT=np.zeros((agmntCount, np.size(sT[:,0]), np.size(sT[0,:])))\n",
    "agmntdT[0,:,:] = sT\n",
    "np.random.seed(0)\n",
    "mu, sigma = 0, delta*100\n",
    "for i in range(1, agmntCount):\n",
    "    agmntdT[i] = agmntdT[0] + np.random.normal(mu, sigma, (np.size(sT[:,0]), np.size(sT[0,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSteps = 1\n",
    "#Keep dataset tail for validate prediction quality\n",
    "cutFromTail = 60\n",
    "#In order to have shifted and unshifted series with same shape\n",
    "t = cutFromTail + predictSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inData = np.zeros((agmntCount, np.size(sT[:-t,0]), np.size(sT[0,:]) + 1))\n",
    "for i in range(0, agmntCount):\n",
    "    inData[i,:,0] = Mode[:-t] / np.amax(Mode[:-t])\n",
    "    inData[i,:,1:]= agmntdT[i,:-t,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = len(inData[0,:,0])\n",
    "train_split = 0.80\n",
    "num_train = int(train_split * num_data)\n",
    "num_test = num_data - num_train\n",
    "inData_train = inData[:, 0:num_train, :]\n",
    "inData_test = inData[:, num_train:, :]\n",
    "num_inData_signals = inData.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-830119b40de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0min_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minData_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutData_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-830119b40de5>\u001b[0m in \u001b[0;36mbatch_generator\u001b[0;34m(batch_size, sequence_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Copy the sequences of data starting at this index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0min_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minData_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midaugmnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mout_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutData_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midaugmnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for outputBlockId in range(0, 10):\n",
    "    outData = np.expand_dims(agmntdT[:,predictSteps:-cutFromTail,outputBlockId], axis=2)\n",
    "    outData_train = outData[:, 0:num_train, :]\n",
    "    outData_test = outData[:, num_train:, :]\n",
    "    num_outData_signals = outData.shape[2]\n",
    "    \n",
    "    def batch_generator(batch_size, sequence_length):\n",
    "        \"\"\"\n",
    "        Generator function for creating random batches of training-data.\n",
    "        \"\"\"\n",
    "        # Infinite loop.\n",
    "        while True:\n",
    "            # Allocate a new array for the batch of input-signals.\n",
    "            in_shape = (batch_size, sequence_length, num_inData_signals)\n",
    "            in_batch = np.zeros(shape=in_shape, dtype=np.float16)\n",
    "\n",
    "            # Allocate a new array for the batch of output-signals.\n",
    "            out_shape = (batch_size, sequence_length, num_outData_signals)\n",
    "            out_batch = np.zeros(shape=out_shape, dtype=np.float16)\n",
    "\n",
    "            # Fill the batch with random sequences of data.\n",
    "            for i in range(batch_size):\n",
    "                # Get a random start-index.\n",
    "                # This points somewhere into the training-data.\n",
    "                idx = np.random.randint(num_train - sequence_length)\n",
    "                # This points somewhere into the augmented series range.\n",
    "                idaugmnt = np.random.randint(agmntCount)\n",
    "            \n",
    "                # Copy the sequences of data starting at this index.\n",
    "                in_batch[i] = inData_train[idaugmnt, idx:idx+sequence_length,:]\n",
    "                out_batch[i] = outData_train[idaugmnt, idx:idx+sequence_length,:]\n",
    "        \n",
    "        yield (in_batch, out_batch)\n",
    "    \n",
    "    batch_size = 128\n",
    "    sequence_length=512\n",
    "    \n",
    "    generator = batch_generator(batch_size=batch_size, sequence_length=sequence_length)\n",
    "    \n",
    "    in_batch, out_batch = next(generator)\n",
    "    validation_data = (np.expand_dims(inData_test[0,:,:], axis=0), np.expand_dims(outData_test[0,:,:], axis=0))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=(None, num_inData_signals,)))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(LSTM(10, return_sequences=True))\n",
    "    model.add(LSTM(3, return_sequences=True))\n",
    "    model.add(Dense(1, activation = \"linear\"))\n",
    "    model.compile(Adam(learning_rate=1e-3), loss='mean_absolute_error')\n",
    "    model.summary()\n",
    "    \n",
    "    path_checkpoint = str(outputBlockId)+'_checkpoint_stacked_LSTM.keras'\n",
    "    callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=False,\n",
    "                                      save_best_only=True)\n",
    "    callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=5, verbose=1)\n",
    "    callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)\n",
    "    callbacks = [callback_early_stopping,\n",
    "            callback_checkpoint,\n",
    "            callback_reduce_lr]\n",
    "    %%time\n",
    "    model.fit_generator(generator=generator,\n",
    "                        epochs=1000,\n",
    "                        steps_per_epoch=50,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=callbacks)\n",
    "    try:\n",
    "        model.load_weights(path_checkpoint)\n",
    "    except Exception as error:\n",
    "        print(\"Error trying to load checkpoint.\")\n",
    "        print(error)\n",
    "    result = model.evaluate(x=np.expand_dims(inData_test[0], axis=0),\n",
    "                        y=np.expand_dims(outData_test[0], axis=0))\n",
    "    print(\"loss (test-set):\", result)\n",
    "    \n",
    "    def plot_comparison(start_idx, length=100, train=True):\n",
    "        \"\"\"\n",
    "        Plot the predicted and true output-signals.\n",
    "    \n",
    "        :param start_idx: Start-index for the time-series.\n",
    "        :param length: Sequence-length to process and plot.\n",
    "        :param train: Boolean whether to use training- or test-set.\n",
    "        \"\"\"\n",
    "    \n",
    "        if train:\n",
    "            # Use training-data.\n",
    "            x = inData_train[0]\n",
    "            y_true = outData_train[0]\n",
    "        else:\n",
    "            # Use test-data.\n",
    "            x = inData_test[0]\n",
    "            y_true = outData_test[0]\n",
    "    \n",
    "        # End-index for the sequences.\n",
    "        end_idx = start_idx + length\n",
    "    \n",
    "        # Select the sequences from the given start-index and\n",
    "        # of the given length.\n",
    "        x = x[start_idx:end_idx]\n",
    "        y_true = y_true[start_idx:end_idx]\n",
    "    \n",
    "        # Input-signals for the model.\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        # Use the model to predict the output-signals.\n",
    "        y_pred = model.predict(x)\n",
    "        print(shape(x))\n",
    "        print(shape(y_pred))\n",
    "        \n",
    "        # Get the output-signal predicted by the model.\n",
    "        signal_pred = y_pred\n",
    "        \n",
    "        # Get the true output-signal from the data-set.\n",
    "        signal_true = y_true\n",
    "    \n",
    "        # Make the plotting-canvas bigger.\n",
    "        plt.figure(figsize=(15,5))\n",
    "        \n",
    "        # Plot and compare the two signals.\n",
    "        plt.plot(signal_true, label='true')\n",
    "        plt.scatter(np.arange(0, len(signal_pred[0])), signal_pred[0], label='pred', color='r')\n",
    "\n",
    "        \n",
    "        # Plot labels etc.\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    plot_comparison(start_idx=1200, length=512, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
